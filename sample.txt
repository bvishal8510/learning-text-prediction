First we must transform the list of input sequences into the form [samples, time steps, features] expected by an LSTM network.

Next we need to rescale the integers to the range 0-to-1 to make the patterns easier to learn by the LSTM network that uses the sigmoid activation function by default.

Finally, we need to convert the output patterns (single characters converted to integers) into a one hot encoding. This is so that we can configure the network to predict the probability of each of the 47 different characters in the vocabulary (an easier representation) rather than trying to force it to predict precisely the next character. Each y value is converted into a sparse vector with a length of 47, full of zeros except with a 1 in the column for the letter (integer) that the pattern represents.

# reshape X to be [samples, time steps, features]
X = numpy.reshape(dataX, (n_patterns, seq_length, 1))
# normalize
X = X / float(n_vocab)
# one hot encode the output variable
y = to_categorical(dataY)
---------------------------------------------------------------------------
NameError                                 Traceback (most recent call last)
<ipython-input-8-7de470329ad8> in <module>()
      4 X = X / float(n_vocab)
      5 # one hot encode the output variable
----> 6 y = to_categorical(dataY)

NameError: name 'to_categorical' is not defined

We can now define our LSTM model. Here we define a single hidden LSTM layer with 256 memory units. The network uses dropout with a probability of 20. The output layer is a Dense layer using the softmax activation function to output a probability prediction for each of the 47 characters between 0 and 1.

The problem is really a single character classification problem with 47 classes and as such is defined as optimizing the log loss (cross entropy), here using the ADAM optimization algorithm for speed.

# define the LSTM model
model = Sequential()
model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))
model.add(Dropout(0.2))
model.add(LSTM(256))
model.add(Dropout(0.2))
model.add(Dense(y.shape[1], activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer='adam')
There is no test dataset. We are modeling the entire training dataset to learn the probability of each character in a sequence.

We are not interested in the most accurate (classification accuracy) model of the training dataset. This would be a model that predicts each character in the training dataset perfectly. Instead we are interested in a generalization of the dataset that minimizes the chosen loss function. We are seeking a balance between generalization and overfitting but short of memorization.

The network is slow to train (about 300 seconds per epoch on an Nvidia K520 GPU). Because of the slowness and because of our optimization requirements, we will use model checkpointing to record all of the network weights to file each time an improvement in loss is observed at the end of the epoch. We will use the best set of weights (lowest loss) to instantiate our generative model in the next section.

# define the checkpoint
filepath="weights-improvement-{epoch:02d}-{loss:.4f}.hdf5"
checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')
callbacks_list = [checkpoint]
We can now fit our model to the data. Here we use a modest number of 20 epochs and a large batch size of 128 patterns.

model.fit(X, y, epochs=50, batch_size=64, callbacks=callbacks_list)
model.fit(X, y, epochs=50, batch_size=64, callbacks=callbacks_list)
Epoch 1/50
144320/144372 [============================>.] - ETA: 0s - loss: 2.8073
Epoch 00001: loss improved from inf to 2.80713, saving model to weights-improvement-01-2.8071.hdf5
144372/144372 [==============================] - 644s 4ms/step - loss: 2.8071
Epoch 2/50
144320/144372 [============================>.] - ETA: 0s - loss: 2.4493
Epoch 00002: loss improved from 2.80713 to 2.44919, saving model to weights-improvement-02-2.4492.hdf5
144372/144372 [==============================] - 644s 4ms/step - loss: 2.4492
Epoch 3/50
144320/144372 [============================>.] - ETA: 0s - loss: 2.2555
Epoch 00003: loss improved from 2.44919 to 2.25546, saving model to weights-improvement-03-2.2555.hdf5
144372/144372 [==============================] - 646s 4ms/step - loss: 2.2555
Epoch 4/50
144320/144372 [============================>.] - ETA: 0s - loss: 2.1219
Epoch 00004: loss improved from 2.25546 to 2.12192, saving model to weights-improvement-04-2.1219.hdf5
144372/144372 [==============================] - 650s 5ms/step - loss: 2.1219
Epoch 5/50
144320/144372 [============================>.] - ETA: 0s - loss: 2.0162
Epoch 00005: loss improved from 2.12192 to 2.01625, saving model to weights-improvement-05-2.0162.hdf5
144372/144372 [==============================] - 652s 5ms/step - loss: 2.0162
Epoch 6/50
144320/144372 [============================>.] - ETA: 0s - loss: 1.9358
Epoch 00006: loss improved from 2.01625 to 1.93575, saving model to weights-improvement-06-1.9358.hdf5
144372/144372 [==============================] - 648s 4ms/step - loss: 1.9358
Epoch 7/50
144320/144372 [============================>.] - ETA: 0s - loss: 1.8694
Epoch 00007: loss improved from 1.93575 to 1.86926, saving model to weights-improvement-07-1.8693.hdf5
144372/144372 [==============================] - 668s 5ms/step - loss: 1.8693
Epoch 8/50
144320/144372 [============================>.] - ETA: 0s - loss: 1.8082
Epoch 00008: loss improved from 1.86926 to 1.80826, saving model to weights-improvement-08-1.8083.hdf5
144372/144372 [==============================] - 665s 5ms/step - loss: 1.8083
Epoch 9/50
144320/144372 [============================>.] - ETA: 0s - loss: 1.7622
Epoch 00009: loss improved from 1.80826 to 1.76216, saving model to weights-improvement-09-1.7622.hdf5
144372/144372 [==============================] - 660s 5ms/step - loss: 1.7622
Epoch 10/50
144320/144372 [============================>.] - ETA: 0s - loss: 1.7154
Epoch 00010: loss improved from 1.76216 to 1.71534, saving model to weights-improvement-10-1.7153.hdf5
144372/144372 [==============================] - 669s 5ms/step - loss: 1.7153
Epoch 11/50
144320/144372 [============================>.] - ETA: 0s - loss: 1.6777
Epoch 00011: loss improved from 1.71534 to 1.67767, saving model to weights-improvement-11-1.6777.hdf5
144372/144372 [==============================] - 665s 5ms/step - loss: 1.6777
Epoch 12/50
144320/144372 [============================>.] - ETA: 0s - loss: 1.6448
Epoch 00012: loss improved from 1.67767 to 1.64469, saving model to weights-improvement-12-1.6447.hdf5
144372/144372 [==============================] - 662s 5ms/step - loss: 1.6447
Epoch 13/50
144320/144372 [============================>.] - ETA: 0s - loss: 1.6166
Epoch 00013: loss improved from 1.64469 to 1.61656, saving model to weights-improvement-13-1.6166.hdf5
144372/144372 [==============================] - 663s 5ms/step - loss: 1.6166
Epoch 14/50
144320/144372 [============================>.] - ETA: 0s - loss: 1.5833
Epoch 00014: loss improved from 1.61656 to 1.58333, saving model to weights-improvement-14-1.5833.hdf5
144372/144372 [==============================] - 660s 5ms/step - loss: 1.5833
Epoch 15/50
144320/144372 [============================>.] - ETA: 0s - loss: 1.5578
Epoch 00015: loss improved from 1.58333 to 1.55770, saving model to weights-improvement-15-1.5577.hdf5
144372/144372 [==============================] - 666s 5ms/step - loss: 1.5577
Epoch 16/50
144320/144372 [============================>.] - ETA: 0s - loss: 1.5339
Epoch 00016: loss improved from 1.55770 to 1.53396, saving model to weights-improvement-16-1.5340.hdf5
144372/144372 [==============================] - 662s 5ms/step - loss: 1.5340
Epoch 17/50
144320/144372 [============================>.] - ETA: 0s - loss: 1.5095
Epoch 00017: loss improved from 1.53396 to 1.50962, saving model to weights-improvement-17-1.5096.hdf5
144372/144372 [==============================] - 662s 5ms/step - loss: 1.5096
Epoch 18/50
144320/144372 [============================>.] - ETA: 0s - loss: 1.4890
Epoch 00018: loss improved from 1.50962 to 1.48902, saving model to weights-improvement-18-1.4890.hdf5
144372/144372 [==============================] - 663s 5ms/step - loss: 1.4890
Epoch 19/50
144320/144372 [============================>.] - ETA: 0s - loss: 1.4700
Epoch 00019: loss improved from 1.48902 to 1.47010, saving model to weights-improvement-19-1.4701.hdf5
144372/144372 [==============================] - 664s 5ms/step - loss: 1.4701
Epoch 20/50
144320/144372 [============================>.] - ETA: 0s - loss: 1.4509
Epoch 00020: loss improved from 1.47010 to 1.45092, saving model to weights-improvement-20-1.4509.hdf5
144372/144372 [==============================] - 666s 5ms/step - loss: 1.4509
Epoch 21/50
144320/144372 [============================>.] - ETA: 0s - loss: 1.4337
Epoch 00021: loss improved from 1.45092 to 1.43376, saving model to weights-improvement-21-1.4338.hdf5
144372/144372 [==============================] - 664s 5ms/step - loss: 1.4338
Epoch 22/50
144320/144372 [============================>.] - ETA: 0s - loss: 1.4190
Epoch 00022: loss improved from 1.43376 to 1.41898, saving model to weights-improvement-22-1.4190.hdf5
144372/144372 [==============================] - 665s 5ms/step - loss: 1.4190
Epoch 23/50
144320/144372 [============================>.] - ETA: 0s - loss: 1.4052
Epoch 00023: loss improved from 1.41898 to 1.40517, saving model to weights-improvement-23-1.4052.hdf5
144372/144372 [==============================] - 665s 5ms/step - loss: 1.4052
Epoch 24/50
144320/144372 [============================>.] - ETA: 0s - loss: 1.3885
Epoch 00024: loss improved from 1.40517 to 1.38852, saving model to weights-improvement-24-1.3885.hdf5
144372/144372 [==============================] - 665s 5ms/step - loss: 1.3885
Epoch 25/50
144320/144372 [============================>.] - ETA: 0s - loss: 1.3796
Epoch 00025: loss improved from 1.38852 to 1.37962, saving model to weights-improvement-25-1.3796.hdf5
144372/144372 [==============================] - 664s 5ms/step - loss: 1.3796
Epoch 26/50
144320/144372 [============================>.] - ETA: 0s - loss: 1.3694
Epoch 00026: loss improved from 1.37962 to 1.36943, saving model to weights-improvement-26-1.3694.hdf5
144372/144372 [==============================] - 665s 5ms/step - loss: 1.3694
Epoch 27/50
144320/144372 [============================>.] - ETA: 0s - loss: 1.3579
Epoch 00027: loss improved from 1.36943 to 1.35786, saving model to weights-improvement-27-1.3579.hdf5
144372/144372 [==============================] - 664s 5ms/step - loss: 1.3579
Epoch 28/50
 12352/144372 [=>............................] - ETA: 10:08 - loss: 1.3062Buffered data was truncated after reaching the output size limit.
Generating text using the trained LSTM network is relatively straightforward.

Firstly, we load the data and define the network in exactly the same way, except the network weights are loaded from a checkpoint file and the network does not need to be trained.

# load the network weights
filename = "weights-improvement-38-1.2788.hdf5"
model.load_weights(filename)
model.compile(loss='categorical_crossentropy', optimizer='adam')
Also, when preparing the mapping of unique characters to integers, we must also create a reverse mapping that we can use to convert the integers back to characters so that we can understand the predictions.

int_to_char = dict((i, c) for i, c in enumerate(chars))
Finally, we need to actually make predictions.

The simplest way to use the Keras LSTM model to make predictions is to first start off with a seed sequence as input, generate the next character then update the seed sequence to add the generated character on the end and trim off the first character. This process is repeated for as long as we want to predict new characters (e.g. a sequence of 1,000 characters in length).

We can pick a random input pattern as our seed sequence, then print generated characters as we generate them.

# pick a random seed
start = numpy.random.randint(0, len(dataX)-1)
pattern = dataX[start]
print ("Seed:")
print( "\"", ''.join([int_to_char[value] for value in pattern]), "\"")
# generate characters
for i in range(100):
    x = numpy.reshape(pattern, (1, len(pattern), 1))
    x = x / float(n_vocab)
    prediction = model.predict(x, verbose=0)
    index = numpy.argmax(prediction)
    result = int_to_char[index]
    seq_in = [int_to_char[value] for value in pattern]
    sys.stdout.write(result)
    pattern.append(index)
    pattern = pattern[1:len(pattern)]
print ("\nDone.")